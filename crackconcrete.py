# -*- coding: utf-8 -*-
"""CrackConcrete.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g-me4XpjtU3afp28Y6i2sR4kg9qlZKEY
"""

#1. Import Packages / library

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, optimizers, losses, metrics, callbacks, applications
from tensorflow.keras.callbacks import TensorBoard, EarlyStopping
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import os

#get dataset from web
!wget https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/5y9wdsg2zt-2.zip

#create folder
!makedir dataset

#images in dataset folder
!unzip "5y9wdsg2zt-2.zip"
!unrar x "Concrete Crack Images for Classification.rar" "dataset"

#3. Data Preparation 
#(A) Define the path to the positive and negative data folder
#PATH = os.path.join(os.getcwd(),"dataset")
#print(PATH)

dataset = 'Negative' , 'Positive'

train_path = os.path.join(os.getcwd(),'dataset')
val_path = os.path.join(os.getcwd(),'dataset')

#(B) Define the batch size and image size
BATCH_SIZE = 64
IMG_SIZE = (160,160)
SEED = 64

#(C) Load the data into tensorflow dataset using the specific method
#train_dataset = tf.keras.utils.image_dataset_from_directory(train_path, shuffle = True, batch_size = BATCH_SIZE, image_size = IMG_SIZE)
train_dataset = tf.keras.utils.image_dataset_from_directory(train_path, validation_split = 0.3, subset = 'training', seed = SEED, shuffle = True,batch_size = BATCH_SIZE, image_size = IMG_SIZE)

#val_dataset = tf.keras.utils.image_dataset_from_directory(val_path, shuffle= True, batch_size = BATCH_SIZE, image_size = IMG_SIZE)
val_dataset = tf.keras.utils.image_dataset_from_directory(val_path, validation_split = 0.3, subset = 'validation', seed = SEED, shuffle = True,batch_size = BATCH_SIZE, image_size = IMG_SIZE)

#4. Display some images as example
class_names = train_dataset.class_names

plt.figure(figsize=(10,10))
for images,labels in train_dataset.take(1):
  for i in range(9):
    plt.subplot(3,3,i+1)
    plt.imshow(images[i].numpy().astype('uint8')) #unsigned 8 bit interger
    plt.title(class_names[labels[i]])
    plt.axis('off')

#5. Further split the validation dataset into validation-test split
val_batches = tf.data.experimental.cardinality(val_dataset)
test_dataset = val_dataset.take(val_batches//5)
validation_dataset = val_dataset.skip(val_batches//5)

#6. Convert the BatchDataSet into PrefetchDataset
AUTOTUNE = tf.data.AUTOTUNE

pf_train = train_dataset.prefetch(buffer_size = AUTOTUNE)
pf_val = validation_dataset.prefetch(buffer_size = AUTOTUNE)
pf_test = test_dataset.prefetch(buffer_size = AUTOTUNE)

#7 create a small pipeline for data augmentation
data_augmentation = keras.Sequential()
data_augmentation.add(layers.RandomFlip('horizontal'))
data_augmentation.add(layers.RandomRotation(0.2))

#8 Apply the data augmentation to test it out
for images, labels in pf_train.take(1):
  first_image = images[0]
  plt.figure(figsize = (10,10))
  for i in range(9):
    plt.subplot(3,3,i+1)
    augmented_image = data_augmentation (tf.expand_dims(first_image, axis =0))
    plt.imshow(augmented_image[0]/255.0)
    plt.axis('off')

#8. Prepare the layer for the data preprocessing
preprocessing_input = applications.mobilenet_v2.preprocess_input

#9. Apply transfer learning
IMG_SHAPE = IMG_SIZE + (3,)
feature_extractor = applications.MobileNetV2(input_shape = IMG_SHAPE, include_top = False, weights = 'imagenet')

#Disable the training for the feature extractor(freeze the layers)
feature_extractor.trainable = False
feature_extractor.summary()
keras.utils.plot_model(feature_extractor, show_shapes= True) #saved to png file

#10. Create the classification layers
global_avg = layers.GlobalAveragePooling2D()
output_layer = layers.Dense(len(class_names), activation = 'softmax')

#11. Use functional API to link all of the modules together, 
inputs = keras.Input(shape = IMG_SHAPE)
x = data_augmentation(inputs)
x = preprocessing_input(x)
x = feature_extractor(x)
x = global_avg(x)
x = layers.Dropout(0.3)(x) # model dont overfit easily with dropout
outputs = output_layer(x)

model = keras.Model(inputs= inputs, outputs = outputs)
model.summary()

#12. Compile the model
optimizer = optimizers.Adam(learning_rate = 0.0001)
loss = losses.SparseCategoricalCrossentropy()
model.compile(optimizer = optimizer, loss = loss, metrics= ['accuracy'])

#Evaluate the model before model training
loss0, accuracy0 = model.evaluate(pf_val)
print("Loss= ", loss0)
print("Accuracy = ", accuracy0)

# Commented out IPython magic to ensure Python compatibility.
#tensorboard
import datetime

LOGS_PATH = os.path.join(os.getcwd(),'logs', datetime.datetime.now().strftime('%Y%m%d-%H%M%S')) #write into folder call 'logs'

ts_callback = TensorBoard(log_dir = LOGS_PATH)
es_callback = EarlyStopping(monitor ='val_loss', patience = 5, verbose =0 , restore_best_weights = True)

# %load_ext tensorboard
!rm -rf ./logs/
# %tensorboard --logdir logs #plot from logs folder

#train the model
EPOCHS = 10
history = model.fit(pf_train, validation_data = pf_val, epochs = EPOCHS, callbacks = [ts_callback, es_callback])

